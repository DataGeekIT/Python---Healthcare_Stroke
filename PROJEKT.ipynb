# Agnieszka K. 
Python 3.12.8

Celem projektu jest analiza danych dotyczących udaru mózgu, znajdujących się w pliku `healthcare-dataset-stroke-data.csv`. Wyniki należy przedstawić w formie  raportu (RMarkdown, Jupyter, etc.) zawierającego kod programu jak i opisy dokonywanych decyzji. Analiza powinna zawierać co najmniej następujące punkty:

1. Czyszczenie danych (usuwanie/inputacja braków danych, naprawa błędów, transformacje danych, rozwiązanie problemu wartości odstających)
2. Eksploracyjna analiza danych
3. Zamodelowanie zmiennej `stroke` na podstawie pozostałych zmiennych. Minimum 3 modele.
4. Ewaluacja na zbiorze testowym (wybór modelu i metryk z uzasadnieniem)


### Import bibliotek i wczytanie danych
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import missingno
from imblearn.over_sampling import SMOTE
from imblearn.ensemble import EasyEnsembleClassifier, BalancedRandomForestClassifier  # dla lepszej obsługi niezrównoważonych danych
import xgboost as xgb
import lightgbm as lgb

df = pd.read_csv("healthcare-dataset-stroke-data.csv")
### Informacje
print("Podgląd danych:")
print(df.head())

print("\nInformacje o danych:")
print(df.info())
>ID - numer indentyfikacyjny osoby,  
>Gender - płeć,  
>Age - wiek,  
>hypertension - parametr zdrowotny, czy osoba ma nadciśnienie,  
>heart_disease - paramter zdrowotny czy osoba cierpi na choroby serca,  
>ever_married - inf osobiste czy osoba jest w związku małżeńskim,  
>work_type - inf osobiste - rodzaj miejsca pracy,  
>Residence_type - inf osobiste - miejsce zamieszkania,  
>avg_glucose_level - parametr zdrowotny - średni poziom glukozy we krwi,  
>bmi - parametr zdrowotny - wskaźnik masy ciała,  
>smoking_status - inf osobiste - obecny status palenia przez osobę,  
>stroke - CEL - czy osoba miała udar?


## 1. Czyszczenie danych
df.info()
round(df.describe(include='all'), 2)
### 1.1 Uzupełnienie braków danych  
Kolumna bmi zawierała brakujące wartości, które zostały uzupełnione medianą. Kolumna id została usunięta, ponieważ nie wnosiła wartości dla analizy.
# Sprawdzenie brakujących danych
missingno.matrix(df)
plt.show()

# Uzupełnianie brakujących wartości w kolumnie 'bmi'
if 'bmi' in df.columns:
    df['bmi'] = df['bmi'].fillna(df['bmi'].median())

# Usunięcie kolumny ID
df.drop(columns='id', inplace=True)

# Weryfikacja zmian
print("Aktualny stan danych:")
print(df.info())
### 1.2 Weryfikacja wartości odstających
# Tworzenie podzbioru danych numerycznych
numeric_cols = ['age', 'bmi', 'avg_glucose_level']
sns.pairplot(df[numeric_cols])
plt.show()

# Macierz korelacji
corr = df[numeric_cols + ['stroke']].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title("Macierz korelacji")
plt.show()

## 2. Eksploracyjna analiza danych
### 2.1 Wpływ BMI na wystąpienie udaru
Poniżj anazliza wpływu BMI na ryzyko wystąpienia udaru
# Klasyfikacja BMI
bmi_bins = [-float('inf'), 18.5, 24.9, 29.9, 34.9, 39.9, float('inf')]
bmi_labels = ['niedowaga', 'waga prawidłowa', 'nadwaga', 'otyłość I stopnia', 'otyłość II stopnia', 'otyłość III stopnia']
df['bmi_category'] = pd.cut(df['bmi'], bins=bmi_bins, labels=bmi_labels, right=False)

# Wykres BMI
plt.figure(figsize=(10, 6))
ax = sns.countplot(x='bmi_category', hue='stroke', data=df, palette='coolwarm')
for p in ax.patches:
    ax.annotate(f'{p.get_height()} ({100 * p.get_height() / len(df):.1f}%)',
                (p.get_x() + p.get_width() / 2., p.get_height()), 
                ha='center', va='center', 
                fontsize=10, color='black', 
                xytext=(0, 5), textcoords='offset points')
plt.title("BMI a wystąpienie udaru")
plt.xlabel("Klasyfikacja BMI")
plt.ylabel("Liczba osób")
plt.legend(title='Wystąpienie udaru', labels=['Zdrowi', 'Z udarem'])
plt.grid(True)
plt.show()

#### Interpretacja:

Osoby z nadwagą i otyłością mają wyraźnie większą liczbę przypadków udaru.
W grupie z prawidłową wagą odsetek przypadków jest mniejszy, co sugeruje wpływ masy ciała na ryzyko udaru.
### 2.1 Analiza grup wiekowych
# Grupowanie wiekowe
age_bins = [0, 16, 32, 49, 65, 82]
age_labels = ['0-16', '17-32', '33-49', '50-65', '66-82']
df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels)

# Wykres grup wiekowych
plt.figure(figsize=(10, 6))
ax = sns.barplot(x='age_group', y='stroke', data=df, estimator=lambda x: sum(x)/len(x)*100, ci=None, palette='coolwarm')
for p in ax.patches:
    ax.annotate(f'{p.get_height():.1f}%', 
                (p.get_x() + p.get_width() / 2., p.get_height()), 
                ha='center', va='center', 
                fontsize=10, color='black', 
                xytext=(0, 5), textcoords='offset points')
plt.title("Odsetek przypadków udaru w grupach wiekowych")
plt.xlabel("Grupy wiekowe")
plt.ylabel("Odsetek udarów (%)")
plt.grid(True)
plt.show()
#### Interpretacja:

Ryzyko udaru wzrasta wraz z wiekiem, szczególnie powyżej 50. roku życia.
## 3. Modelowanie
### 3.1 Podział danych
# Podział na cechy i zmienną celu
X = df.drop(columns=['stroke'])
y = df['stroke']

# Kolumny numeryczne i kategoryczne
numeric_cols = ['age', 'avg_glucose_level', 'bmi']
categorical_cols = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']

# Podział na dane treningowe i testowe
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
### 3.2 Przekrztałcanie danych
# Transformacje danych
numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), 
                                      ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), 
                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_cols),
                                               ('cat', categorical_transformer, categorical_cols)])

# 3 modele 
models = {
    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),
    'XGBoost': xgb.XGBClassifier(scale_pos_weight=10, random_state=42),
    'LightGBM': lgb.LGBMClassifier(class_weight='balanced', random_state=42)
}
# Zrównoważenie danych za pomocą SMOTE (tylko na danych treningowych)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(preprocessor.fit_transform(X_train), y_train)

# Przywracam nazwy kolumn po zastosowaniu SMOTE
X_resampled_df = pd.DataFrame(X_resampled, columns=preprocessor.get_feature_names_out())
### 3.3 Trenowanie modeli i ewaluacja
# Dla każdego modelu przeprowadzam trening, predykcję i ocenę
for model_name, model in models.items():
    print(f"Ocena modelu: {model_name}")
    
    # Trening modelu
    model.fit(X_resampled_df, y_resampled)
    
    # Predykcja na danych testowych
    X_test_transformed = preprocessor.transform(X_test)
    y_test_pred = model.predict(X_test_transformed)
    y_test_prob = model.predict_proba(X_test_transformed)[:, 1]  # Prawdopodobieństwo klasy 1
    
    # Raport klasyfikacji
    print(classification_report(y_test, y_test_pred))
    
    # Obliczanie AUC-ROC
    auc_roc = roc_auc_score(y_test, y_test_prob)
    print(f"AUC-ROC: {auc_roc:.4f}")
    
    # Rysowanie wykresu ROC
    fpr, tpr, _ = roc_curve(y_test, y_test_prob)
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_roc:.2f})', linestyle='-', linewidth=2)

# Dodanie etykiety na wykresie ROC
plt.text(0.6, 0.2, f'AUC-ROC dla modeli', fontsize=12)
plt.plot([0, 1], [0, 1], 'k--', label='Losowy podział')
plt.title('Krzywa ROC', fontsize=16)
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.legend()
plt.grid()
plt.show()
#### Wnioski:  

Random Forest ma bardzo wysoką precyzję dla klasy 0, ale bardzo słabo wykrywa klasy 1, co jest typowym problemem dla niezrównoważonych danych. AUC-ROC pokazuje, że model działa lepiej niż losowy podział, ale wciąż nie jest to idealne rozwiązanie.  

Logistic Regression wypadła lepiej w rozpoznawaniu klasy 1, mimo niskiej precyzji dla tej klasy. Jest to typowe, gdy model dobrze wychwytuje rzadkie przypadki, ale może generować fałszywe alarmy (fałszywe pozytywy).  

Gradient Boosting ma nieco lepsze wykrywanie klasy 1 niż Random Forest, ale nadal nie jest to idealny model. Wykazuje lepszą skuteczność w ocenie ogólnej niż Random Forest  
